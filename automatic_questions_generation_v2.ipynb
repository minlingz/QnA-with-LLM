{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json, os, sys\n",
    "import os\n",
    "import re\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(chunk, client, model=\"gpt-4o-mini\"):\n",
    "    prompt = f\"\"\"\n",
    "You are an AI assistant analyzing a corpus of NIPS papers spanning multiple years. \n",
    "Your task is to design multiple-choice questions (MCQs) that highlight key differences among those papers. \n",
    "I will provide abstracts of NIPS papers, and you will generate unique, comprehensive MCQs based on them. \n",
    "\n",
    "These questions should:\n",
    "- Help to differentiate between the papers.\n",
    "- Are common to all articles, not specific to any one paper.\n",
    "- Cover various aspects, including but not limited to research focus, methodology, theoretical advancements, and applications.\n",
    "- Are unique and not repeated in any other batch.\n",
    "- Include \"None of the above\" as one of the options for each question.\n",
    "\n",
    "Below is the format and example I need:\n",
    "\n",
    "1. What is the primary focus of the paper?\n",
    "   - A. Reinforcement learning\n",
    "   - B. Deep learning\n",
    "   - C. Bayesian methods\n",
    "   - D. Kernel methods\n",
    "   - E. None of the above\n",
    "\n",
    "2. What is the primary focus of the paper?\n",
    "   - A. Faster convergence compared to existing methods\n",
    "   - B. Higher accuracy\n",
    "   - C. Better scalability to large datasets\n",
    "   - D. Improved interpretability of results\n",
    "   - E. None of the above\n",
    "\n",
    "Generate as many unique and comprehensive questions as possible based on the given paper abstracts below. \n",
    "Each question should be designed to highlight key differences between the papers.\n",
    "\n",
    "Abstracts: {chunk}\n",
    "\n",
    "\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model, messages=messages, temperature=0\n",
    "        )\n",
    "\n",
    "        content = response.choices[0].message.content\n",
    "        return content\n",
    "    except Exception as e:  # if the model fails to return a response\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"Sorry, error from GPT.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batches(input_file, output_file, client, model):\n",
    "    # Read the article summaries from the input JSON file\n",
    "    with open(input_file, \"r\") as f:\n",
    "        articles = json.load(f)\n",
    "\n",
    "    # Shuffle the data\n",
    "    random.shuffle(articles)\n",
    "\n",
    "    # Extract summaries from the articles\n",
    "    article_summaries = [article[\"abstract_new\"] for article in articles]\n",
    "    num_summaries = len(article_summaries)\n",
    "    batch_size = 100\n",
    "\n",
    "    responses = []\n",
    "\n",
    "    # Process the article summaries in batches of 100\n",
    "    for i in range(0, num_summaries, batch_size):\n",
    "        batch = article_summaries[i : i + batch_size]\n",
    "        chunk = json.dumps(batch)  # Convert the batch to a JSON string\n",
    "\n",
    "        questions = generate_questions(chunk, client, model=model)\n",
    "        responses.append(\n",
    "            {\n",
    "                \"batch_start\": i,\n",
    "                \"batch_end\": min(i + batch_size - 1, num_summaries - 1),\n",
    "                \"questions\": questions,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Write the generated questions to the output JSON file\n",
    "    with open(output_file, \"w\") as f:\n",
    "        json.dump(responses, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = f\"json/topic_{topic}.json\"\n",
    "output_file = \"json/output_questions_v2.json\"\n",
    "\n",
    "# Assuming you have your OpenAI API client initialized as `client`\n",
    "process_batches(input_file, output_file, client, model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_questions(batch_questions):\n",
    "    questions = {}\n",
    "    q_count = 1\n",
    "    for batch in batch_questions:\n",
    "        batch_qs = batch[\"questions\"].split(\"\\n\\n\")\n",
    "        for q in batch_qs:\n",
    "            match = re.match(\n",
    "                r\"\\d+\\. (.+?)\\n\\s+- A\\. (.+?)\\n\\s+- B\\. (.+?)\\n\\s+- C\\. (.+?)\\n\\s+- D\\. (.+?)\\n\\s+- E\\. (.+?)$\",\n",
    "                q.strip(),\n",
    "                re.DOTALL,\n",
    "            )\n",
    "            if match:\n",
    "                question = match.group(1).strip()\n",
    "                choices = {\n",
    "                    \"A\": match.group(2).strip(),\n",
    "                    \"B\": match.group(3).strip(),\n",
    "                    \"C\": match.group(4).strip(),\n",
    "                    \"D\": match.group(5).strip(),\n",
    "                    \"E\": match.group(6).strip(),\n",
    "                }\n",
    "                if question not in questions:\n",
    "                    questions[question] = choices\n",
    "    return questions\n",
    "\n",
    "\n",
    "def convert_to_json2_format(unique_questions):\n",
    "    formatted_questions = {}\n",
    "    q_num = 1\n",
    "    for question, choices in unique_questions.items():\n",
    "        formatted_questions[f\"Q{q_num}\"] = {\"question\": question, \"choices\": choices}\n",
    "        q_num += 1\n",
    "    return formatted_questions\n",
    "\n",
    "\n",
    "def process_json(input_json):\n",
    "    unique_questions = extract_questions(input_json)\n",
    "    return convert_to_json2_format(unique_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON 1\n",
    "with open(\"json/output_questions_v2.json\", \"r\") as f:\n",
    "    json1 = json.load(f)\n",
    "\n",
    "# Process JSON 1 to get JSON 2 format\n",
    "json2_format = process_json(json1)\n",
    "\n",
    "# Save the result to a new JSON file\n",
    "with open(f\"json/topic_{topic}_formated_output_questions_v2.json\", \"w\") as f:\n",
    "    json.dump(json2_format, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
