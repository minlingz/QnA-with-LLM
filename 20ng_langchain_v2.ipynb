{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 (optional): Generate summary of a corpus of documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain-based pipeline for MCQ-based document clustering\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "import json\n",
    "import ast\n",
    "\n",
    "\n",
    "def load_documents(json_path):\n",
    "    with open(json_path, \"r\") as f:\n",
    "        docs = json.load(f)\n",
    "    return [doc[\"text\"].strip().replace(\"\\n\", \" \") for doc in docs]\n",
    "\n",
    "\n",
    "def extract_json_block(text):\n",
    "    start = text.find(\"[\")\n",
    "    end = text.rfind(\"]\")\n",
    "    if start != -1 and end != -1:\n",
    "        return text[start : end + 1]\n",
    "    return text\n",
    "\n",
    "\n",
    "def generate_summary(docs, llm, size=1000):\n",
    "    # randomly sample 1000 documents for the prompt, you can change the number as long as it is within the limit of the LLM input token limit\n",
    "    np.random.seed(123)\n",
    "    indices = np.random.choice(len(docs), size=size, replace=False)\n",
    "    docs = [docs[i] for i in indices]\n",
    "    # join the documents into a single string\n",
    "    joined_text = \"\\n\\n\".join(docs[:size])\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Given the following corpus of {size} documents, please summarize the main topics discussed in the documents. The documents are separated by two newlines.\n",
    "\n",
    "        Format your response as a valid JSON object like this:\n",
    "\n",
    "        {{\"summary\": \"Main topic summary\"}}\n",
    "        Only return the JSON — no prose or commentary.\n",
    "\n",
    "        --Documents--:\n",
    "        {joined_text}\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = chain.run(joined_text=joined_text, size=size)\n",
    "    print(\"Raw LLM output:\", response)\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return ast.literal_eval(extract_json_block(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xr/lfv3bx8j69x4n2wzsd6ryvp40000gn/T/ipykernel_37256/4175139594.py:7: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm_summary = ChatOpenAI(\n",
      "/var/folders/xr/lfv3bx8j69x4n2wzsd6ryvp40000gn/T/ipykernel_37256/3100556967.py:46: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n",
      "/var/folders/xr/lfv3bx8j69x4n2wzsd6ryvp40000gn/T/ipykernel_37256/3100556967.py:47: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chain.run(joined_text=joined_text, size=size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM output: {\"summary\": \"The main topics discussed in this corpus of 1000 documents are North American professional sports, with a strong focus on hockey (NHL) and baseball (MLB) in the early 1990s. The documents include detailed game summaries, player statistics, standings, and box scores for both sports, as well as discussions about team performance, player trades, and predictions for playoff outcomes. There is significant conversation about television and radio coverage of games, fan experiences, and the impact of league expansion and realignment. Other recurring themes include debates over player value, Hall of Fame candidacies, the merits of various statistical measures, and the influence of management decisions. The corpus also contains discussions about the representation of European players in the NHL, the role of women in sports, and the intersection of sports with broader cultural and social issues. Additionally, there are references to college and minor league hockey, fantasy sports pools, and the logistics of attending games. Overall, the documents reflect a highly engaged sports fan community sharing information, opinions, and humor about contemporary sports events and issues.\"}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # This loads variables from .env into os.environ\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm_summary = ChatOpenAI(\n",
    "    model_name=\"gpt-4.1\", temperature=0\n",
    ")  # here we use gpt-4.1 for summarization size its context length is larger than gpt-4o\n",
    "docs = load_documents(\"json/20ng/topic_9_10_documents.json\")\n",
    "summary = generate_summary(docs=docs, llm=llm_summary, size=1000)\n",
    "# save the summary to a file\n",
    "with open(\"json/20ng/topic_9_10_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Based on the summary, draft your own instruction in the prompt for LLM to generate MCQs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_mcqs(docs, llm, size=100):\n",
    "    # randomly sample 100 documents for the prompt, you can change the number as long as it is within the limit of the LLM input token limit\n",
    "    np.random.seed(42)\n",
    "    indices = np.random.choice(len(docs), size=size, replace=False)\n",
    "    docs = [docs[i] for i in indices]\n",
    "    # join the documents into a single string\n",
    "    joined_text = \"\\n\\n\".join(docs[:size])\n",
    "    prompt = PromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Given the following sports-related forum posts:\n",
    "        {joined_text}\n",
    "\n",
    "        It contains a diverse set of text entries from what appears to be sports discussions, possibly from forums or mailing lists, mostly centered around hockey and baseball.\n",
    "\n",
    "        Generate 3 multiple-choice questions that help distinguish subtopics, such as hockey or baseball of discussion.\n",
    "\n",
    "        Each question should have 4 options (A, B, C, D), including “None of the above” as one of the answer choices for every question.\n",
    "\n",
    "        Format your response as a valid JSON array like this:\n",
    "\n",
    "        [\n",
    "          {{\"question\": \"What is the main topic of the post?\", \"options\": [\"A. Player analysis\", \"B. Media complaints\", \"C. Statistics discussion\", \"D. None of the above\"]}},\n",
    "          ...\n",
    "        ]\n",
    "        Only return the JSON — no prose or commentary.\n",
    "        \"\"\"\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt)\n",
    "    response = chain.run(joined_text=joined_text)\n",
    "    print(\"Raw LLM output:\", response)\n",
    "    try:\n",
    "        return json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        return ast.literal_eval(extract_json_block(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw LLM output: ```json\n",
      "[\n",
      "  {\"question\": \"What sport is primarily discussed in the majority of the posts?\", \"options\": [\"A. Baseball\", \"B. Hockey\", \"C. Basketball\", \"D. None of the above\"]},\n",
      "  {\"question\": \"Which player is mentioned in relation to a significant injury update?\", \"options\": [\"A. Nolan Ryan\", \"B. Steve Howe\", \"C. Eli Manning\", \"D. None of the above\"]},\n",
      "  {\"question\": \"What type of statistics are frequently referenced in the discussions?\", \"options\": [\"A. Player batting averages\", \"B. Goalie save percentages\", \"C. Team standings\", \"D. None of the above\"]}\n",
      "]\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "llm_mcqs = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "mcqs = generate_mcqs(docs, llm=llm_mcqs, size=100)\n",
    "\n",
    "# save mcqs to json file\n",
    "with open(\"json/20ng/topic_9_10_mcqs.json\", \"w\") as f:\n",
    "    json.dump(mcqs, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
